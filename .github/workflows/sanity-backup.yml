name: Sanity Daily Backup

on:
  schedule:
    - cron: '0 3 * * *'  # Daily at 3:00 AM UTC
  workflow_dispatch:      # Manual trigger

jobs:
  backup:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'

      - name: Create backup script
        run: |
          cat > backup.js << 'ENDOFFILE'
          import https from 'https';
          import fs from 'fs';
          import path from 'path';
          import { createWriteStream } from 'fs';
          import { pipeline } from 'stream/promises';
          
          const PROJECT_ID = process.env.SANITY_PROJECT_ID;
          const TOKEN = process.env.SANITY_TOKEN;
          const DATASET = 'production';
          
          async function downloadFile(url, dest) {
            return new Promise((resolve, reject) => {
              https.get(url, (response) => {
                if (response.statusCode === 302 || response.statusCode === 301) {
                  downloadFile(response.headers.location, dest).then(resolve).catch(reject);
                  return;
                }
                const file = createWriteStream(dest);
                response.pipe(file);
                file.on('finish', () => {
                  file.close();
                  resolve();
                });
                file.on('error', (err) => {
                  fs.unlink(dest, () => reject(err));
                });
              }).on('error', reject);
            });
          }
          
          async function exportData() {
            console.log('Exporting documents...');
            
            // Export documents via Sanity HTTP API
            const exportUrl = `https://${PROJECT_ID}.api.sanity.io/v1/data/export/${DATASET}`;
            
            await new Promise((resolve, reject) => {
              https.get(exportUrl, {
                headers: { 'Authorization': `Bearer ${TOKEN}` }
              }, (response) => {
                const file = createWriteStream('data.ndjson');
                response.pipe(file);
                file.on('finish', () => {
                  file.close();
                  console.log('Documents exported successfully');
                  resolve();
                });
                file.on('error', reject);
              }).on('error', reject);
            });
            
            // Parse documents and extract asset references
            console.log('Parsing assets...');
            const data = fs.readFileSync('data.ndjson', 'utf8');
            const lines = data.split('\n').filter(line => line.trim());
            
            const assetRefs = new Set();
            for (const line of lines) {
              try {
                const doc = JSON.parse(line);
                const jsonStr = JSON.stringify(doc);
                
                // Find all image asset references
                const matches = jsonStr.matchAll(/"_ref":"(image-[a-f0-9]+-\d+x\d+-[a-z]+)"/g);
                for (const match of matches) {
                  assetRefs.add(match[1]);
                }
              } catch (e) {
                // Skip invalid lines
              }
            }
            
            console.log(`Found ${assetRefs.size} unique assets`);
            
            // Create assets directory
            if (!fs.existsSync('assets')) {
              fs.mkdirSync('assets');
            }
            
            // Download all assets from CDN
            let downloaded = 0;
            for (const ref of assetRefs) {
              try {
                // Convert ref to CDN URL: image-abc123-1920x1080-jpg -> abc123-1920x1080.jpg
                const parts = ref.replace('image-', '').split('-');
                const hash = parts[0];
                const dimensions = parts[1];
                const ext = parts[2];
                const filename = `${hash}-${dimensions}.${ext}`;
                const url = `https://cdn.sanity.io/images/${PROJECT_ID}/${DATASET}/${filename}`;
                
                await downloadFile(url, path.join('assets', filename));
                downloaded++;
                if (downloaded % 10 === 0) {
                  console.log(`Downloaded ${downloaded}/${assetRefs.size} assets...`);
                }
              } catch (err) {
                console.error(`Failed to download ${ref}: ${err.message}`);
              }
            }
            
            console.log(`Successfully downloaded ${downloaded} assets`);
          }
          
          exportData().catch(console.error);
          ENDOFFILE
          
          # Create minimal package.json for ES modules support
          echo '{"type": "module"}' > package.json

      - name: Create backup wrapper script
        run: |
          cat > backup-all.js << 'ENDOFFILE'
          import { exec } from 'child_process';
          import { promisify } from 'util';
          import fs from 'fs';
          
          const execAsync = promisify(exec);
          const markets = JSON.parse(process.env.SANITY_MARKETS);
          
          console.log(`Starting backup for ${markets.length} markets...`);
          
          async function backupMarket(market) {
            const marketSlug = market.market.toLowerCase().replace(/\s+/g, '-');
            console.log(`\n📦 Processing ${market.market}...`);
            
            try {
              // Run backup script
              const { stdout, stderr } = await execAsync('node backup.js', {
                env: {
                  ...process.env,
                  SANITY_PROJECT_ID: market.id,
                  SANITY_TOKEN: market.key
                }
              });
              
              if (stdout) console.log(stdout);
              if (stderr) console.error(stderr);
              
              // Create archive
              const timestamp = new Date().toISOString().replace(/[:.]/g, '-').slice(0, 19);
              const filename = `sanity-backup-${marketSlug}-${timestamp}.tar.gz`;
              
              await execAsync(`tar -czf ${filename} data.ndjson assets/`);
              
              console.log(`✅ ${market.market} backup completed: ${filename}`);
              
              // Cleanup for next market
              await execAsync('rm -rf data.ndjson assets/');
              
              return { market: market.market, filename, success: true };
            } catch (error) {
              console.error(`❌ ${market.market} backup failed:`, error.message);
              // Cleanup on error
              try { await execAsync('rm -rf data.ndjson assets/'); } catch {}
              return { market: market.market, error: error.message, success: false };
            }
          }
          
          // Process markets with concurrency limit
          async function processWithConcurrency(items, concurrency, fn) {
            const results = [];
            const executing = [];
            
            for (const item of items) {
              const promise = fn(item).then(result => {
                executing.splice(executing.indexOf(promise), 1);
                return result;
              });
              results.push(promise);
              executing.push(promise);
              
              if (executing.length >= concurrency) {
                await Promise.race(executing);
              }
            }
            
            return Promise.all(results);
          }
          
          // Run backups with max 5 concurrent
          const results = await processWithConcurrency(markets, 5, backupMarket);
          
          console.log('\n\n📊 Backup Summary:');
          console.log('==================');
          const successful = results.filter(r => r.success);
          const failed = results.filter(r => !r.success);
          
          console.log(`✅ Successful: ${successful.length}`);
          console.log(`❌ Failed: ${failed.length}`);
          
          if (failed.length > 0) {
            console.log('\nFailed markets:');
            failed.forEach(r => console.log(`  - ${r.market}: ${r.error}`));
          }
          
          // List all created backups
          const { stdout } = await execAsync('ls -lh sanity-backup-*.tar.gz 2>/dev/null || echo "No backups created"');
          console.log('\n📦 Created backups:');
          console.log(stdout);
          ENDOFFILE

      - name: Run backups for all markets
        run: node backup-all.js
        env:
          SANITY_MARKETS: ${{ secrets.SANITY_MARKETS }}

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}

      - name: Upload all backups to S3
        run: |
          echo "📤 Uploading backups to S3..."
          
          # Upload each backup to its market-specific folder
          for BACKUP_FILE in sanity-backup-*.tar.gz; do
            if [ -f "$BACKUP_FILE" ]; then
              # Extract market name from filename: sanity-backup-spain-2025-10-28... -> spain
              MARKET_SLUG=$(echo "$BACKUP_FILE" | sed 's/sanity-backup-\([^-]*\)-.*/\1/')
              
              echo "📦 Uploading $BACKUP_FILE to $MARKET_SLUG folder..."
              aws s3 cp "$BACKUP_FILE" "s3://${{ secrets.S3_BUCKET }}/backups/${MARKET_SLUG}/${BACKUP_FILE}"
              
              echo "✅ Uploaded: s3://${{ secrets.S3_BUCKET }}/backups/${MARKET_SLUG}/${BACKUP_FILE}"
            fi
          done
          
          echo ""
          echo "🎉 All backups uploaded successfully!"
